{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import boto3\n",
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class CODE_ENV(Enum):\n",
    "    EC2=0 #Running in AWS EC2\n",
    "    DEV=1 #Running in IOT Device\n",
    "    WSL=2 #Running in PC Wsl Environment\n",
    "#print(list(CODE_ENV))\n",
    "\n",
    "class DATASET_ID(Enum):\n",
    "    First=0\n",
    "    Second=1\n",
    "    Third=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_paths(code_env:CODE_ENV)->dict:\n",
    "\n",
    "    #Step1 : set the pre-defined root folder paths\n",
    "    if code_env == CODE_ENV.EC2:\n",
    "        #To access 's3' without any access key embedded folloWSLg dependencies shall be met:\n",
    "        # 1. Policy for user : Allow-S3-Passrole-to-EC2, AmazonS3FullAccess\n",
    "        # 2. Role            : S3Admin\n",
    "        aws_s3 = boto3.resource('s3')\n",
    "        s3_bucket = aws_s3.Bucket('anomaly-detection-from-bearing-vibration-project-bucket')\n",
    "        s3_bucket_objects=[]\n",
    "        for s3_bucket_object in s3_bucket.objects.all():\n",
    "            s3_bucket_objects.append(s3_bucket_object)\n",
    "    elif code_env == CODE_ENV.WSL:\n",
    "        ##################################################################################\n",
    "        #SET PATH to DATASET in your PC (till but not including folder named '1st')\n",
    "        #CUSTOMIZE AS per your PC setup\n",
    "        #curr_dir=os.getcwd()        \n",
    "        curr_dir = str(Path('/mnt/g/My Drive/CDS/github/CDS/capstone_project/'))\n",
    "        dataset_root_path =Path('')\n",
    "        dataset_root_path1 = Path(curr_dir+'/'+'capstone-data/01_PHM-Bearing')\n",
    "        dataset_root_path2 = Path(curr_dir+'/'+'models/capstone-data/01_PHM-Bearing')\n",
    "        if dataset_root_path1.is_dir():\n",
    "            dataset_root_path = dataset_root_path1\n",
    "        elif dataset_root_path2.is_dir():\n",
    "            dataset_root_path = dataset_root_path2\n",
    "        else:\n",
    "            print('Path ERROR!!!', str(dataset_root_path1), str(dataset_root_path2))\n",
    "        ##################################################################################\n",
    "        \n",
    "    \n",
    "    #Step2 : collect 3 dataset file details\n",
    "    if code_env == CODE_ENV.EC2:\n",
    "        s3_objects_1st_dataset=[]\n",
    "        s3_objects_2nd_dataset=[]\n",
    "        s3_objects_3rd_dataset=[]\n",
    "        paths = []\n",
    "        for s3_object in s3_bucket_objects:\n",
    "            path_parts = Path(s3_object.key).parts\n",
    "            if len(path_parts) == 4 and path_parts[0] == 'data_input' and path_parts[1] == 'IMS':\n",
    "                paths.append(s3_object)\n",
    "                if path_parts[2] == '1st_test':\n",
    "                    s3_objects_1st_dataset.append(s3_object)\n",
    "                elif path_parts[2] == '2nd_test':\n",
    "                    s3_objects_2nd_dataset.append(s3_object)\n",
    "                else:\n",
    "                    s3_objects_3rd_dataset.append(s3_object)\n",
    "    elif code_env == CODE_ENV.WSL:\n",
    "        data_set1_path = dataset_root_path.as_posix() + '/1st_test'\n",
    "        data_set2_path = dataset_root_path.as_posix() + '/2nd_test'\n",
    "        data_set3_path = dataset_root_path.as_posix() + '/3rd_test'\n",
    "        filelist_1st_dataset = [data_set1_path+'/'+src_path for src_path in sorted(os.listdir(data_set1_path))]\n",
    "        filelist_2nd_dataset = [data_set2_path+'/'+src_path for src_path in sorted(os.listdir(data_set2_path))]\n",
    "        filelist_3rd_dataset = [data_set3_path+'/'+src_path for src_path in sorted(os.listdir(data_set3_path))]\n",
    "    \n",
    "    #Step3 : Consolidate to structure => Output\n",
    "    #1st Set has 8 \n",
    "    col_names_1st = ['b1_ch1', 'b1_ch2', 'b2_ch3', 'b2_ch4', 'b3_ch5', 'b3_ch6', 'b4_ch7', 'b4_ch8']\n",
    "    #2nd and 3rd has 4\n",
    "    col_names_2nd_3rd = ['b1_ch1', 'b2_ch2', 'b3_ch3', 'b4_ch4']\n",
    "\n",
    "    dataset_details = {}\n",
    "    dataset_details[DATASET_ID.First]  = {'col_names':col_names_1st}\n",
    "    dataset_details[DATASET_ID.Second] = {'col_names':col_names_2nd_3rd}\n",
    "    dataset_details[DATASET_ID.Third]  = {'col_names':col_names_2nd_3rd}\n",
    "\n",
    "    if code_env == CODE_ENV.EC2:\n",
    "        dataset_details[DATASET_ID.First]['paths']=s3_objects_1st_dataset\n",
    "        dataset_details[DATASET_ID.Second]['paths']=s3_objects_2nd_dataset\n",
    "        dataset_details[DATASET_ID.Third]['paths']=s3_objects_3rd_dataset\n",
    "        #Verify variables\n",
    "        #print('Number of files in 1st Dataset:', len(dataset_details[DATASET_ID.First]['paths']), 'first file=', dataset_details[DATASET_ID.First]['paths'][0].key)\n",
    "        #print('Number of files in 2nd Dataset:', len(dataset_details[DATASET_ID.Second]['paths']), 'first file=', dataset_details[DATASET_ID.Second]['paths'][0].key)\n",
    "        #print('Number of files in 3rd Dataset:', len(dataset_details[DATASET_ID.Third]['paths']), 'first file=', dataset_details[DATASET_ID.Third]['paths'][0].key)\n",
    "\n",
    "    elif code_env == CODE_ENV.WSL:\n",
    "        dataset_details[DATASET_ID.First]['paths']=filelist_1st_dataset\n",
    "        dataset_details[DATASET_ID.Second]['paths']=filelist_2nd_dataset\n",
    "        dataset_details[DATASET_ID.Third]['paths']=filelist_3rd_dataset\n",
    "        #Verify variables\n",
    "        #print('Number of files in 1st Dataset:', len(dataset_details[DATASET_ID.First]['paths']), 'first file=', dataset_details[DATASET_ID.First]['paths'][0])\n",
    "        #print('Number of files in 2nd Dataset:', len(dataset_details[DATASET_ID.Second]['paths']), 'first file=', dataset_details[DATASET_ID.Second]['paths'][0])\n",
    "        #print('Number of files in 3rd Dataset:', len(dataset_details[DATASET_ID.Third]['paths']), 'first file=', dataset_details[DATASET_ID.Third]['paths'][0])\n",
    "    \n",
    "    return dataset_details\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(dataset_details:dict, dataset:DATASET_ID, file_index:int, code_env:CODE_ENV):\n",
    "    df = pd.DataFrame()\n",
    "    if code_env == CODE_ENV.EC2:\n",
    "        s3_object = dataset_details[dataset]['paths'][file_index]\n",
    "        data = s3_object.get()['Body'].read()\n",
    "        df = pd.read_csv(BytesIO(data), header=None, delimiter='\\t', names=dataset_details[dataset]['col_names'], low_memory='False')\n",
    "    elif code_env == CODE_ENV.WSL:\n",
    "        df = pd.read_csv(dataset_details[dataset]['paths'][file_index], header=None, delimiter='\\t', names=dataset_details[dataset]['col_names'], low_memory='False')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    select_input_stepsize= 3000\n",
    "\n",
    "    #####################################################################################\n",
    "    #***************IMP: Update coding environment********************\n",
    "    #####################################################################################\n",
    "    code_env = CODE_ENV.WSL\n",
    "\n",
    "    #Trial: collect filepath details\n",
    "    dataset_details = get_dataset_paths(code_env)\n",
    "\n",
    "    #Trial: Reading content of file\n",
    "    df = get_df(dataset_details, DATASET_ID.First, 0, code_env)\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
