{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import boto3\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<CODE_ENV.EC2: 0>, <CODE_ENV.DEV: 1>, <CODE_ENV.WIN: 2>]\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "class CODE_ENV(Enum):\n",
    "    EC2=0 #Running in AWS EC2\n",
    "    DEV=1 #Running in IOT Device\n",
    "    WIN=2 #Running in Win\n",
    "print(list(CODE_ENV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_paths(code_env:CODE_ENV)->dict:\n",
    "\n",
    "    #Step1 : set the pre-defined root folder paths\n",
    "    if code_env == CODE_ENV.EC2:\n",
    "        #To access 's3' without any access key embedded following dependencies shall be met:\n",
    "        # 1. Policy for user : Allow-S3-Passrole-to-EC2, AmazonS3FullAccess\n",
    "        # 2. Role            : S3Admin\n",
    "        aws_s3 = boto3.resource('s3')\n",
    "        s3_bucket = aws_s3.Bucket('anomaly-detection-from-bearing-vibration-project-bucket')\n",
    "        s3_bucket_objects=[]\n",
    "        for s3_bucket_object in s3_bucket.objects.all():\n",
    "            s3_bucket_objects.append(s3_bucket_object)\n",
    "    elif code_env == CODE_ENV.WIN:\n",
    "        curr_dir=os.getcwd()\n",
    "        dataset_root_path =''\n",
    "        dataset_root_path1 = Path(curr_dir+'/'+'capstone-data/01_PHM-Bearing')\n",
    "        dataset_root_path2 = Path(curr_dir+'/'+'models/capstone-data/01_PHM-Bearing')\n",
    "        if dataset_root_path1.is_dir():\n",
    "            dataset_root_path = dataset_root_path1\n",
    "        elif dataset_root_path2.is_dir():\n",
    "            dataset_root_path = dataset_root_path2\n",
    "        else:\n",
    "            print('Path ERROR!!!', str(dataset_root_path1), str(dataset_root_path2))\n",
    "        \n",
    "    \n",
    "    #Step2 : collect 3 dataset file details\n",
    "    if code_env == CODE_ENV.EC2:\n",
    "        s3_objects_1st_dataset=[]\n",
    "        s3_objects_2nd_dataset=[]\n",
    "        s3_objects_3rd_dataset=[]\n",
    "        paths = []\n",
    "        for s3_object in s3_bucket_objects:\n",
    "            path_parts = Path(s3_object.key).parts\n",
    "            if len(path_parts) == 4 and path_parts[0] == 'data_input' and path_parts[1] == 'IMS':\n",
    "                paths.append(s3_object)\n",
    "                if path_parts[2] == '1st_test':\n",
    "                    s3_objects_1st_dataset.append(s3_object)\n",
    "                elif path_parts[2] == '2nd_test':\n",
    "                    s3_objects_2nd_dataset.append(s3_object)\n",
    "                else:\n",
    "                    s3_objects_3rd_dataset.append(s3_object)\n",
    "    elif code_env == CODE_ENV.WIN:\n",
    "        data_set1_path = dataset_root_path.as_posix() + '/1st_test'\n",
    "        data_set2_path = dataset_root_path.as_posix() + '/2nd_test'\n",
    "        data_set3_path = dataset_root_path.as_posix() + '/3rd_test'\n",
    "        filelist_1st_dataset = [data_set1_path+'/'+src_path for src_path in sorted(os.listdir(data_set1_path))]\n",
    "        filelist_2nd_dataset = [data_set2_path+'/'+src_path for src_path in sorted(os.listdir(data_set2_path))]\n",
    "        filelist_3rd_dataset = [data_set3_path+'/'+src_path for src_path in sorted(os.listdir(data_set3_path))]\n",
    "    \n",
    "    #Step3 : Consolidate to structure => Output\n",
    "    #1st Set has 8 \n",
    "    col_names_1st = ['b1_ch1', 'b1_ch2', 'b2_ch3', 'b2_ch4', 'b3_ch5', 'b3_ch6', 'b4_ch7', 'b4_ch8']\n",
    "    #2nd and 3rd has 4\n",
    "    col_names_2nd_3rd = ['b1_ch1', 'b2_ch2', 'b3_ch3', 'b4_ch4']\n",
    "\n",
    "    dataset_details = {}\n",
    "    dataset_details['1st'] = {'col_names':col_names_1st}\n",
    "    dataset_details['2nd'] = {'col_names':col_names_2nd_3rd}\n",
    "    dataset_details['3rd'] = {'col_names':col_names_2nd_3rd}\n",
    "\n",
    "    if code_env == CODE_ENV.EC2:\n",
    "        dataset_details['1st']['paths']=s3_objects_1st_dataset\n",
    "        dataset_details['2nd']['paths']=s3_objects_2nd_dataset\n",
    "        dataset_details['3rd']['paths']=s3_objects_3rd_dataset\n",
    "        #Verify variables\n",
    "        print('Number of files in 1st Dataset:', len(dataset_details['1st']['paths']), 'first file=', dataset_details['1st']['paths'][0].key)\n",
    "        print('Number of files in 2nd Dataset:', len(dataset_details['2nd']['paths']), 'first file=', dataset_details['2nd']['paths'][0].key)\n",
    "        print('Number of files in 3rd Dataset:', len(dataset_details['3rd']['paths']), 'first file=', dataset_details['3rd']['paths'][0].key)\n",
    "\n",
    "    elif code_env == CODE_ENV.WIN:\n",
    "        dataset_details['1st']['paths']=filelist_1st_dataset\n",
    "        dataset_details['2nd']['paths']=filelist_2nd_dataset\n",
    "        dataset_details['3rd']['paths']=filelist_3rd_dataset\n",
    "        #Verify variables\n",
    "        print('Number of files in 1st Dataset:', len(dataset_details['1st']['paths']), 'first file=', dataset_details['1st']['paths'][0])\n",
    "        print('Number of files in 2nd Dataset:', len(dataset_details['2nd']['paths']), 'first file=', dataset_details['2nd']['paths'][0])\n",
    "        print('Number of files in 3rd Dataset:', len(dataset_details['3rd']['paths']), 'first file=', dataset_details['3rd']['paths'][0])\n",
    "    \n",
    "    return dataset_details\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(dataset_details:dict, dataset:str, file_index:int, code_env:CODE_ENV):\n",
    "    df = pd.DataFrame()\n",
    "    if code_env == CODE_ENV.EC2:\n",
    "        s3_object = dataset_details[dataset]['paths'][file_index]\n",
    "        data = s3_object.get()['Body'].read()\n",
    "        df = pd.read_csv(BytesIO(data), header=None, delimiter='\\t', names=dataset_details[dataset]['col_names'], low_memory='False')\n",
    "    elif code_env == CODE_ENV.WIN:\n",
    "        df = pd.read_csv(dataset_details[dataset]['paths'][file_index], header=None, delimiter='\\t', names=dataset_details[dataset]['col_names'], low_memory='False')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in 1st Dataset: 2156 first file= data_input/IMS/1st_test/2003.10.22.12.06.24\n",
      "Number of files in 2nd Dataset: 984 first file= data_input/IMS/2nd_test/2004.02.12.10.32.39\n",
      "Number of files in 3rd Dataset: 6324 first file= data_input/IMS/3rd_test/2004.03.04.09.27.46\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    select_input_stepsize= 3000\n",
    "\n",
    "    #####################################################################################\n",
    "    #***************IMP: Update coding environment********************\n",
    "    #####################################################################################\n",
    "    code_env = CODE_ENV.EC2\n",
    "\n",
    "    #Trial: collect filepath details\n",
    "    dataset_details = get_dataset_paths(code_env)\n",
    "\n",
    "    #Trial: Reading content of file\n",
    "    df = get_df(dataset_details, '1st', 0, code_env)\n",
    "    df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
