{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import boto3\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<CODE_ENV.EC2: 0>, <CODE_ENV.DEV: 1>, <CODE_ENV.WIN: 2>]\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "class CODE_ENV(Enum):\n",
    "    EC2=0 #Running in AWS EC2\n",
    "    DEV=1 #Running in IOT Device\n",
    "    WIN=2 #Running in Win\n",
    "   \n",
    "print(list(CODE_ENV))\n",
    "\n",
    "#IMP: Update coding environment\n",
    "code_env = CODE_ENV.WIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if code_env == CODE_ENV.EC2:\n",
    "    #To access 's3' without any access key embedded following dependencies shall be met:\n",
    "    # 1. Policy for user : Allow-S3-Passrole-to-EC2, AmazonS3FullAccess\n",
    "    # 2. Role            : S3Admin\n",
    "\n",
    "    aws_s3 = boto3.resource('s3')\n",
    "    s3_bucket = aws_s3.Bucket('anomaly-detection-from-bearing-vibration-project-bucket')\n",
    "\n",
    "    s3_bucket_objects=[]\n",
    "    for s3_bucket_object in s3_bucket.objects.all():\n",
    "        s3_bucket_objects.append(s3_bucket_object)\n",
    "\n",
    "elif code_env == CODE_ENV.WIN:\n",
    "    curr_dir=os.getcwd()\n",
    "    dataset_root_path = Path(curr_dir+'/'+'capstone-data/01_PHM-Bearing')\n",
    "    if not dataset_root_path.is_dir():\n",
    "        print('Path ERROR!!!', str(dataset_root_path))\n",
    "\n",
    "elif code_env == CODE_ENV.DEV:\n",
    "    pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in 1st Dataset: 2156 first file= g:/My Drive/github/bearing-vibration-anomaly-detection/models/capstone-data/01_PHM-Bearing/1st_test/2003.10.22.12.06.24\n",
      "Number of files in 2nd Dataset: 984 first file= g:/My Drive/github/bearing-vibration-anomaly-detection/models/capstone-data/01_PHM-Bearing/2nd_test/2004.02.12.10.32.39\n",
      "Number of files in 3rd Dataset: 6324 first file= g:/My Drive/github/bearing-vibration-anomaly-detection/models/capstone-data/01_PHM-Bearing/3rd_test/2004.03.04.09.27.46\n"
     ]
    }
   ],
   "source": [
    "if code_env == CODE_ENV.EC2:\n",
    "    s3_objects_1st_dataset=[]\n",
    "    s3_objects_2nd_dataset=[]\n",
    "    s3_objects_3rd_dataset=[]\n",
    "    paths = []\n",
    "\n",
    "    for s3_object in s3_bucket_objects:\n",
    "        path_parts = Path(s3_object.key).parts\n",
    "        if len(path_parts) == 4 and path_parts[0] == 'data_input' and path_parts[1] == 'IMS':\n",
    "            paths.append(s3_object)\n",
    "            if path_parts[2] == '1st_test':\n",
    "                s3_objects_1st_dataset.append(s3_object)\n",
    "            elif path_parts[2] == '2nd_test':\n",
    "                s3_objects_2nd_dataset.append(s3_object)\n",
    "            else:\n",
    "                s3_objects_3rd_dataset.append(s3_object)\n",
    "\n",
    "    print('Number of files in 1st Dataset:', len(s3_objects_1st_dataset), 'first file=', s3_objects_1st_dataset[0].key)\n",
    "    print('Number of files in 2nd Dataset:', len(s3_objects_2nd_dataset), 'first file=', s3_objects_2nd_dataset[0].key)\n",
    "    print('Number of files in 3rd Dataset:', len(s3_objects_3rd_dataset), 'first file=', s3_objects_3rd_dataset[0].key)\n",
    "\n",
    "elif code_env == CODE_ENV.WIN:\n",
    "    data_set1_path = dataset_root_path.as_posix() + '/1st_test'\n",
    "    data_set2_path = dataset_root_path.as_posix() + '/2nd_test'\n",
    "    data_set3_path = dataset_root_path.as_posix() + '/3rd_test'\n",
    "    filelist_1st_dataset = [data_set1_path+'/'+src_path for src_path in sorted(os.listdir(data_set1_path))]\n",
    "    filelist_2nd_dataset = [data_set2_path+'/'+src_path for src_path in sorted(os.listdir(data_set2_path))]\n",
    "    filelist_3rd_dataset = [data_set3_path+'/'+src_path for src_path in sorted(os.listdir(data_set3_path))]\n",
    "    \n",
    "    print('Number of files in 1st Dataset:', len(filelist_1st_dataset), 'first file=', filelist_1st_dataset[0])\n",
    "    print('Number of files in 2nd Dataset:', len(filelist_2nd_dataset), 'first file=', filelist_2nd_dataset[0])\n",
    "    print('Number of files in 3rd Dataset:', len(filelist_3rd_dataset), 'first file=', filelist_3rd_dataset[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in 1st Dataset: 2156 first file= g:/My Drive/github/bearing-vibration-anomaly-detection/models/capstone-data/01_PHM-Bearing/1st_test/2003.10.22.12.06.24\n",
      "Number of files in 2nd Dataset: 984 first file= g:/My Drive/github/bearing-vibration-anomaly-detection/models/capstone-data/01_PHM-Bearing/2nd_test/2004.02.12.10.32.39\n",
      "Number of files in 3rd Dataset: 6324 first file= g:/My Drive/github/bearing-vibration-anomaly-detection/models/capstone-data/01_PHM-Bearing/3rd_test/2004.03.04.09.27.46\n"
     ]
    }
   ],
   "source": [
    "#1st Set has 8 \n",
    "col_names_1st = ['b1_ch1', 'b1_ch2', 'b2_ch3', 'b2_ch4', 'b3_ch5', 'b3_ch6', 'b4_ch7', 'b4_ch8']\n",
    "#2nd and 3rd has 4\n",
    "col_names_2nd_3rd = ['b1_ch1', 'b2_ch2', 'b3_ch3', 'b4_ch4']\n",
    "\n",
    "col_names_set = [col_names_1st, col_names_2nd_3rd, col_names_2nd_3rd]\n",
    "select_columns = [\n",
    "        [\n",
    "            ['b1_ch1', 'b1_ch2'],\n",
    "            ['b2_ch3', 'b2_ch4'],\n",
    "            ['b3_ch5', 'b3_ch6'],\n",
    "            ['b4_ch7', 'b4_ch8']\n",
    "        ],\n",
    "        [\n",
    "            ['b1_ch1'],\n",
    "            ['b2_ch2'],\n",
    "            ['b3_ch3'],\n",
    "            ['b4_ch4'],\n",
    "        ],\n",
    "        [\n",
    "            ['b1_ch1'],\n",
    "            ['b2_ch2'],\n",
    "            ['b3_ch3'],\n",
    "            ['b4_ch4'],    \n",
    "        ],\n",
    "    ]\n",
    "\n",
    "data_set_paths=[]\n",
    "if code_env == CODE_ENV.EC2:\n",
    "    data_set_paths= [s3_objects_1st_dataset, s3_objects_2nd_dataset, s3_objects_3rd_dataset]\n",
    "    #Verify variables\n",
    "    print('Number of files in 1st Dataset:', len(data_set_paths[0]), 'first file=', data_set_paths[0][0].key)\n",
    "    print('Number of files in 2nd Dataset:', len(data_set_paths[1]), 'first file=', data_set_paths[1][0].key)\n",
    "    print('Number of files in 3rd Dataset:', len(data_set_paths[2]), 'first file=', data_set_paths[2][0].key)\n",
    "\n",
    "elif code_env == CODE_ENV.WIN:\n",
    "    data_set_paths= [filelist_1st_dataset, filelist_2nd_dataset, filelist_3rd_dataset]\n",
    "    #Verify variables\n",
    "    print('Number of files in 1st Dataset:', len(data_set_paths[0]), 'first file=', data_set_paths[0][0])\n",
    "    print('Number of files in 2nd Dataset:', len(data_set_paths[1]), 'first file=', data_set_paths[1][0])\n",
    "    print('Number of files in 3rd Dataset:', len(data_set_paths[2]), 'first file=', data_set_paths[2][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b1_ch1</th>\n",
       "      <th>b1_ch2</th>\n",
       "      <th>b2_ch3</th>\n",
       "      <th>b2_ch4</th>\n",
       "      <th>b3_ch5</th>\n",
       "      <th>b3_ch6</th>\n",
       "      <th>b4_ch7</th>\n",
       "      <th>b4_ch8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   b1_ch1  b1_ch2  b2_ch3  b2_ch4  b3_ch5  b3_ch6  b4_ch7  b4_ch8\n",
       "0  -0.022  -0.039  -0.183  -0.054  -0.105  -0.134  -0.129  -0.142\n",
       "1  -0.105  -0.017  -0.164  -0.183  -0.049   0.029  -0.115  -0.122\n",
       "2  -0.183  -0.098  -0.195  -0.125  -0.005  -0.007  -0.171  -0.071\n",
       "3  -0.178  -0.161  -0.159  -0.178  -0.100  -0.115  -0.112  -0.078\n",
       "4  -0.208  -0.129  -0.261  -0.098  -0.151  -0.205  -0.063  -0.066"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_data_set = 0\n",
    "select_input_stepsize= 3000\n",
    "\n",
    "#Trial: Reading content of file\n",
    "df = pd.DataFrame()\n",
    "if code_env == CODE_ENV.EC2:\n",
    "    s3_object = data_set_paths[0][0]\n",
    "    data = s3_object.get()['Body'].read()\n",
    "    df = pd.read_csv(BytesIO(data), header=None, delimiter='\\t', names=col_names_set[0], low_memory='False')\n",
    "elif code_env == CODE_ENV.WIN:\n",
    "    df = pd.read_csv(data_set_paths[0][0], header=None, delimiter='\\t', names=col_names_set[0], low_memory='False')\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
