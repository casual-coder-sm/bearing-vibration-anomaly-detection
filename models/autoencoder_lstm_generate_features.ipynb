{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "seed(10)\n",
    "tf.random.set_seed(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_feedinput_pipeline\n",
    "from model_feedinput_pipeline import CODE_ENV, DATASET_ID\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root Mean Squared Sum\n",
    "def calculate_rms(df):\n",
    "    result = []\n",
    "    for col in df:\n",
    "        r = np.sqrt((df[col]**2).sum() / len(df[col]))\n",
    "        result.append(r)\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "# extract peak-to-peak features\n",
    "def calculate_p2p(df):\n",
    "    return np.array(df.max().abs() + df.min().abs())\n",
    "\n",
    "\n",
    "# extract shannon entropy (cut signals to 500 bins)\n",
    "def calculate_entropy(df):\n",
    "    ent = []\n",
    "    for col in df:\n",
    "        ent.append(entropy(pd.cut(df[col], 500).value_counts()))\n",
    "    return np.array(ent)\n",
    "\n",
    "\n",
    "# extract clearence factor\n",
    "def calculate_clearence(df):\n",
    "    result = []\n",
    "    for col in df:\n",
    "        r = ((np.sqrt(df[col].abs())).sum() / len(df[col]))**2\n",
    "        result.append(r)\n",
    "    return np.array(result)\n",
    "\n",
    "def get_time_feature(code_env: CODE_ENV, dataset_details, id:DATASET_ID, fileindex:int, select_columns:list):\n",
    "    filepath = dataset_details[id]['paths'][fileindex]\n",
    "    \n",
    "    #step1: get raw_data and associated filename\n",
    "    raw_data = model_feedinput_pipeline.get_df(dataset_details, id, fileindex, code_env)\n",
    "    raw_data = raw_data[select_columns]\n",
    "    if code_env == CODE_ENV.EC2:\n",
    "        filename = Path(filepath.key).name\n",
    "    elif code_env == CODE_ENV.WSL:\n",
    "        filename =  Path(filepath).name\n",
    "    \n",
    "    #step2 : Generate features\n",
    "    mean_abs = raw_data.abs().mean().to_numpy().reshape(1,len(select_columns))\n",
    "    std = raw_data.std().to_numpy().reshape(1,len(select_columns))\n",
    "    skew = raw_data.skew().to_numpy().reshape(1,len(select_columns))\n",
    "    kurtosis = raw_data.kurtosis().to_numpy().reshape(1,len(select_columns))\n",
    "    entropy = calculate_entropy(raw_data).reshape(1,len(select_columns))\n",
    "    rms = calculate_rms(raw_data).reshape(1,len(select_columns))\n",
    "    max_abs=raw_data.abs().max().to_numpy().reshape(1,len(select_columns))\n",
    "    p2p = calculate_p2p(raw_data).reshape(1,len(select_columns))\n",
    "    crest = (max_abs/rms)\n",
    "    clearence = calculate_clearence(raw_data).reshape(1,len(select_columns))\n",
    "    shape = (rms/mean_abs)\n",
    "    impulse = (max_abs/mean_abs)\n",
    "\n",
    "    #step3 : save to dataframe\n",
    "    df_mean_abs = pd.DataFrame(mean_abs, columns=[c+'_mean' for c in select_columns])\n",
    "    df_std = pd.DataFrame(std, columns=[c+'_std'  for c in select_columns])\n",
    "    df_skew = pd.DataFrame(skew, columns=[c+'_skew' for c in select_columns])\n",
    "    df_kurtosis = pd.DataFrame(kurtosis, columns=[c+'_kurtosis' for c in select_columns])\n",
    "    df_entropy = pd.DataFrame(entropy, columns=[c+'_entropy' for c in select_columns])\n",
    "    df_rms = pd.DataFrame(rms, columns=[c+'_rms' for c in select_columns])        \n",
    "    df_max = pd.DataFrame(max_abs, columns=[c+'_max' for c in select_columns])\n",
    "    df_p2p = pd.DataFrame(p2p, columns=[c+'_p2p' for c in select_columns])\n",
    "    df_crest = pd.DataFrame(crest, columns=[c+'_crest' for c in select_columns])\n",
    "    df_clearence = pd.DataFrame(clearence, columns=[c+'_clearence' for c in select_columns])\n",
    "    df_shape = pd.DataFrame(shape, columns=[c+'_shape' for c in select_columns])\n",
    "    df_impulse = pd.DataFrame(impulse, columns=[c+'_impulse' for c in select_columns])\n",
    "    df_filename = pd.DataFrame([filename], columns=['filename'])\n",
    "    df = pd.concat([df_filename, df_mean_abs, df_std, df_skew, df_kurtosis\n",
    "                        ,df_entropy, df_rms, df_max, df_p2p, df_crest\n",
    "                        ,df_clearence, df_shape, df_impulse]\n",
    "                        ,axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_time_features(code_env: CODE_ENV, dataset_details, id:DATASET_ID, select_columns:list):\n",
    "    #time_features = ['mean','std','skew','kurtosis','entropy','rms','max','p2p', 'crest', 'clearence', 'shape', 'impulse']\n",
    "    \n",
    "    data = pd.DataFrame()   \n",
    "    for fileindex, filepath in enumerate(dataset_details[id]['paths']):\n",
    "        #get time feature\n",
    "        df = get_time_feature(code_env, dataset_details, id, fileindex, select_columns)\n",
    "        #concat with previous set\n",
    "        data = pd.concat([data, df], axis=0)\n",
    "        #interactive reporting of progress\n",
    "        if fileindex % 10 == 0:\n",
    "            print('Processed ', fileindex, ' out of ', len(dataset_details[id]['paths']) )\n",
    "\n",
    "    data['filename'] = pd.to_datetime(data['filename'], format='%Y.%m.%d.%H.%M.%S')\n",
    "    data = data.set_index('filename')\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #####################################################################################\n",
    "    #***************IMP: Update coding environment********************\n",
    "    #####################################################################################\n",
    "    code_env = CODE_ENV.WSL    \n",
    "    curr_dataset = DATASET_ID.Second\n",
    "    \n",
    "    #Step 1 : Setup Data Source\n",
    "    dataset_paths = model_feedinput_pipeline.get_dataset_paths(code_env)\n",
    "\n",
    "    #Step 2 : Generate time features for the specified dataset and columns\n",
    "    select_input_stepsize= 3000\n",
    "    selected_columns = ['b1_ch1', 'b2_ch2', 'b3_ch3', 'b4_ch4']\n",
    "    if curr_dataset == DATASET_ID.First:\n",
    "        select_columns = {\n",
    "            1 : ['b1_ch1', 'b2_ch3', 'b3_ch5', 'b4_ch7'],\n",
    "            2 : ['b1_ch2', 'b2_ch4', 'b3_ch6', 'b4_ch8'],\n",
    "        }\n",
    "        selected_columns = select_columns[2]\n",
    "    print('columns chosen for training = ', selected_columns)\n",
    "    time_feature_data = get_time_features(code_env, dataset_paths, curr_dataset, selected_columns)\n",
    "\n",
    "    #Step 3 : Save Output\n",
    "    time_feature_data_filename=['timefeatures_1st.csv', 'timefeatures_2nd.csv', 'timefeatures_3rd.csv']\n",
    "    time_feature_data.to_csv(time_feature_data_filename[curr_dataset.value])\n",
    "    merged_data = pd.read_csv(time_feature_data_filename[curr_dataset.value])\n",
    "    merged_data.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
