{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-21 16:27:13.388010: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from  sys import path as sys_path\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "#check in the order sub-directory to main-directory\n",
    "if 'autoencoder_lstm' in  curr_dir:\n",
    "    os.chdir('..')\n",
    "if 'models' in curr_dir in curr_dir:\n",
    "    pass\n",
    "elif 'bearing-vibration-anomaly-detection' in curr_dir:\n",
    "    os.chdir('./models')\n",
    "curr_dir = os.getcwd()\n",
    "sys_path.insert(0, curr_dir)\n",
    "\n",
    "import model_feedinput_pipeline\n",
    "from model_feedinput_pipeline import CODE_ENV, DATASET_ID\n",
    "from models.autoencoder_lstm.autoencoder_lstm_main import scale_timefeature_data, prepare_lstm_input, pred_test_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys    \n",
    "\n",
    "#####################################################################################\n",
    "#***************IMP: Update coding environment********************\n",
    "#####################################################################################\n",
    "code_env = CODE_ENV.WSL    \n",
    "curr_dataset = DATASET_ID.Third\n",
    "\n",
    "\n",
    "#Step 1 : Setup Data Source\n",
    "dataset_paths = model_feedinput_pipeline.get_dataset_paths(code_env)   \n",
    "\n",
    "#Step 2 : Read the time features generated earlier\n",
    "time_feature_data_filename=['timefeatures_1st_1.csv', 'timefeatures_1st_2.csv',\n",
    "                            'timefeatures_2nd.csv', 'timefeatures_3rd.csv']\n",
    "\n",
    "tf_file_indx = 0\n",
    "cut_off_date_time = '2003-11-20 23:54:03'\n",
    "if curr_dataset == DATASET_ID.First:\n",
    "    tf_file_indx = 0 #0 or 1\n",
    "    cut_off_date_time = '2003-11-20 23:54:03'\n",
    "elif curr_dataset == DATASET_ID.Second:\n",
    "    tf_file_indx = 2\n",
    "    cut_off_date_time = '2004-02-15 12:52:39'\n",
    "elif curr_dataset == DATASET_ID.Third:\n",
    "    tf_file_indx = 3\n",
    "    cut_off_date_time = '2004-04-08 23:51:57'\n",
    "\n",
    "time_features_data = pd.read_csv(time_feature_data_filename[tf_file_indx])\n",
    "print('Number of records in TimeFeatureData=', len(time_features_data))\n",
    "\n",
    "restored_model = tf.keras.models.load_model('./bvad_ae_lstm')\n",
    "\n",
    "\n",
    "#Step 1 : Re-construct the index columns 'date_time'    \n",
    "#print(time_features_data.describe().T)\n",
    "#print(time_features_data.columns)\n",
    "time_features_data=time_features_data.rename(columns={'filename':'date_time'})\n",
    "time_features_data['date_time']=pd.to_datetime(time_features_data['date_time'])   \n",
    "\n",
    "#Step 2 : Prepare Train, Validation and Test test\n",
    "train = time_features_data[time_features_data['date_time'] <= cut_off_date_time]\n",
    "test  = time_features_data[time_features_data['date_time'] > cut_off_date_time]\n",
    "train = train.set_index('date_time')\n",
    "test  = test.set_index('date_time')\n",
    "\n",
    "#Step 3 : Prepare Data : Normalize & Reshape\n",
    "X_train, X_test = scale_timefeature_data(train, test)\n",
    "X_train, X_test = prepare_lstm_input(X_train, X_test)\n",
    "\n",
    "threshold = 1.8828531646427422\n",
    "print('Threshold =', threshold)\n",
    "# calculate the same metrics for the training set \n",
    "# and merge all data in a single dataframe for plotting\n",
    "test_scored1, X_test_pred1, XTest1 = pred_test_autoencoder(restored_model, X_train, train, threshold)\n",
    "test_scored2, X_test_pred2, XTest2 = pred_test_autoencoder(restored_model, X_test, test, threshold)\n",
    "scored = pd.concat([test_scored1, test_scored2])\n",
    "\n",
    "#print(scored)\n",
    "\n",
    "# plot bearing failure time plot\n",
    "import matplotlib.pyplot as plt\n",
    "scored.plot(logy=True,  figsize=(16,9), ylim=[1e-2,1e2], color=['blue','red'])\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
