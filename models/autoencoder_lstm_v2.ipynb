{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_feedinput_pipeline\n",
    "from model_feedinput_pipeline import CODE_ENV, DATASET_ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_input_stepsize= 3000\n",
    "select_columns = {\n",
    "        DATASET_ID.First : ['b1_ch1', 'b2_ch3', 'b3_ch5', 'b4_ch7'],\n",
    "        DATASET_ID.Second: ['b1_ch1', 'b2_ch2', 'b3_ch3', 'b4_ch4'],\n",
    "        DATASET_ID.Third : ['b1_ch1', 'b2_ch2', 'b3_ch3', 'b4_ch4']\n",
    "}\n",
    "#####################################################################################\n",
    "#***************IMP: Update coding environment********************\n",
    "#####################################################################################\n",
    "code_env = CODE_ENV.EC2\n",
    "\n",
    "dataset_paths = model_feedinput_pipeline.get_dataset_paths(code_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "seed(10)\n",
    "tf.random.set_seed(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root Mean Squared Sum\n",
    "def calculate_rms(df):\n",
    "    result = []\n",
    "    for col in df:\n",
    "        r = np.sqrt((df[col]**2).sum() / len(df[col]))\n",
    "        result.append(r)\n",
    "    return result\n",
    "\n",
    "\n",
    "# extract peak-to-peak features\n",
    "def calculate_p2p(df):\n",
    "    return np.array(df.max().abs() + df.min().abs())\n",
    "\n",
    "\n",
    "# extract shannon entropy (cut signals to 500 bins)\n",
    "def calculate_entropy(df):\n",
    "    ent = []\n",
    "    for col in df:\n",
    "        ent.append(entropy(pd.cut(df[col], 500).value_counts()))\n",
    "    return np.array(ent)\n",
    "\n",
    "\n",
    "# extract clearence factor\n",
    "def calculate_clearence(df):\n",
    "    result = []\n",
    "    for col in df:\n",
    "        r = ((np.sqrt(df[col].abs())).sum() / len(df[col]))**2\n",
    "        result.append(r)\n",
    "    return result\n",
    "\n",
    "\n",
    "def time_features(dataset_details, id:model_feedinput_pipeline.DATASET_ID, select_columns:dict):\n",
    "    time_features = ['mean','std','skew','kurtosis','entropy','rms','max','p2p', 'crest', 'clearence', 'shape', 'impulse']\n",
    "    tf_columns = [c+'_'+tf for c in ['B1','B2','B3','B4'] for tf in time_features]\n",
    "    \n",
    "    data = pd.DataFrame(columns=dataset_details[id]['col_names'])\n",
    "    data = data[select_columns[id]]\n",
    "    for fileindex in range(len(dataset_details[id]['paths'])):\n",
    "        raw_data = model_feedinput_pipeline.get_df(dataset_details, id, fileindex, code_env)\n",
    "        mean_abs = np.array(raw_data.abs().mean()).reshape(-1,1)\n",
    "        std = np.array(raw_data.std()).reshape(-1, 1)\n",
    "        skew = np.array(raw_data.skew()).reshape(-1, 1)\n",
    "        kurtosis = np.array(raw_data.kurtosis()).reshape(-1, 1)\n",
    "        entropy = calculate_entropy(raw_data).reshape(-1, 1)\n",
    "        rms = np.array(calculate_rms(raw_data)).reshape(-1, 1)\n",
    "        max_abs = np.array(raw_data.abs().max()).reshape(-1, 1)\n",
    "        p2p = calculate_p2p(raw_data).reshape(-1, 1)\n",
    "        crest = max_abs/rms\n",
    "        clearence = np.array(calculate_clearence(raw_data)).reshape(-1, 1)\n",
    "        shape = rms / mean_abs\n",
    "        impulse = max_abs / mean_abs\n",
    "\n",
    "        \n",
    "\n",
    "        mean_abs = pd.DataFrame(mean_abs, columns=[c+'_mean' for c in tf_columns])\n",
    "        std = pd.DataFrame(std, columns=[c+'_std' for c in tf_columns])\n",
    "        skew = pd.DataFrame(skew, columns=[c+'_skew' for c in tf_columns])\n",
    "        kurtosis = pd.DataFrame(kurtosis, columns=[c+'_kurtosis' for c in tf_columns])\n",
    "        entropy = pd.DataFrame(entropy, columns=[c+'_entropy' for c in tf_columns])\n",
    "        rms = pd.DataFrame(rms, columns=[c+'_rms' for c in tf_columns])\n",
    "        max_abs = pd.DataFrame(max_abs, columns=[c+'_max' for c in tf_columns])\n",
    "        p2p = pd.DataFrame(p2p, columns=[c+'_p2p' for c in tf_columns])\n",
    "        crest = pd.DataFrame(crest, columns=[c+'_crest' for c in tf_columns])\n",
    "        clearence = pd.DataFrame(clearence, columns=[c+'_clearence' for c in tf_columns])\n",
    "        shape = pd.DataFrame(shape, columns=[c+'_shape' for c in tf_columns])\n",
    "        impulse = pd.DataFrame(impulse, columns=[c+'_impulse' for c in tf_columns])\n",
    "\n",
    "        mean_abs.index = [filename]\n",
    "        std.index = [filename]\n",
    "        skew.index = [filename]\n",
    "        kurtosis.index = [filename]\n",
    "        entropy.index = [filename]\n",
    "        rms.index = [filename]\n",
    "        max_abs.index = [filename]\n",
    "        p2p.index = [filename]\n",
    "        crest.index = [filename]\n",
    "        clearence.index = [filename]\n",
    "        shape.index = [filename]\n",
    "        impulse.index = [filename] \n",
    "\n",
    "        merge = pd.concat([mean_abs, std, skew, kurtosis, entropy, rms, max_abs, p2p,crest,clearence, shape, impulse], axis=1)\n",
    "        data = data.append(merge)\n",
    "\n",
    "    cols = [c+'_'+tf for c in cols2 for tf in time_features]\n",
    "    data = data[cols]\n",
    "\n",
    "    data.index = pd.to_datetime(data.index, format='%Y.%m.%d.%H.%M.%S')\n",
    "    data = data.sort_index()\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = time_features(dataset_paths, model_feedinput_pipeline.DATASET_ID.First)\n",
    "set1.to_csv('set1_timefeatures.csv')\n",
    "\n",
    "merged_data = pd.read_csv(\"./set1_timefeatures.csv\")\n",
    "merged_data = merged_data.rename(columns={'Unnamed: 0':'time'})\n",
    "merged_data.set_index('time')\n",
    "merged_data.describe()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
