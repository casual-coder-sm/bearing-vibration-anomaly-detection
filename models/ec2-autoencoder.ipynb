{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_is_ec2 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_is_ec2 == True:\n",
    "    #To access 's3' without any access key embedded following dependencies shall be met:\n",
    "    # 1. Policy for user : Allow-S3-Passrole-to-EC2, AmazonS3FullAccess\n",
    "    # 2. Role            : S3Admin\n",
    "\n",
    "    aws_s3 = boto3.resource('s3')\n",
    "    s3_bucket = aws_s3.Bucket('anomaly-detection-from-bearing-vibration-project-bucket')\n",
    "\n",
    "    s3_bucket_objects=[]\n",
    "    for s3_bucket_object in s3_bucket.objects.all():\n",
    "        s3_bucket_objects.append(s3_bucket_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in 1st Dataset: 2156 first file= data_input/IMS/1st_test/2003.10.22.12.06.24\n",
      "Number of files in 2nd Dataset: 984 first file= data_input/IMS/2nd_test/2004.02.12.10.32.39\n",
      "Number of files in 3rd Dataset: 6324 first file= data_input/IMS/3rd_test/2004.03.04.09.27.46\n"
     ]
    }
   ],
   "source": [
    "if flag_is_ec2 == True:\n",
    "    s3_objects_1st_dataset=[]\n",
    "    s3_objects_2nd_dataset=[]\n",
    "    s3_objects_3rd_dataset=[]\n",
    "    paths = []\n",
    "\n",
    "    for s3_object in s3_bucket_objects:\n",
    "        path_parts = Path(s3_object.key).parts\n",
    "        if len(path_parts) == 4 and path_parts[0] == 'data_input' and path_parts[1] == 'IMS':\n",
    "            paths.append(s3_object)\n",
    "            if path_parts[2] == '1st_test':\n",
    "                s3_objects_1st_dataset.append(s3_object)\n",
    "            elif path_parts[2] == '2nd_test':\n",
    "                s3_objects_2nd_dataset.append(s3_object)\n",
    "            else:\n",
    "                s3_objects_3rd_dataset.append(s3_object)\n",
    "\n",
    "    print('Number of files in 1st Dataset:', len(s3_objects_1st_dataset), 'first file=', s3_objects_1st_dataset[0].key)\n",
    "    print('Number of files in 2nd Dataset:', len(s3_objects_2nd_dataset), 'first file=', s3_objects_2nd_dataset[0].key)\n",
    "    print('Number of files in 3rd Dataset:', len(s3_objects_3rd_dataset), 'first file=', s3_objects_3rd_dataset[0].key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in 1st Dataset: 2156 first file= data_input/IMS/1st_test/2003.10.22.12.06.24\n",
      "Number of files in 2nd Dataset: 984 first file= data_input/IMS/2nd_test/2004.02.12.10.32.39\n",
      "Number of files in 3rd Dataset: 6324 first file= data_input/IMS/3rd_test/2004.03.04.09.27.46\n"
     ]
    }
   ],
   "source": [
    "#1st Set has 8 \n",
    "col_names_1st = ['b1_ch1', 'b1_ch2', 'b2_ch3', 'b2_ch4', 'b3_ch5', 'b3_ch6', 'b4_ch7', 'b4_ch8']\n",
    "#2nd and 3rd has 4\n",
    "col_names_2nd_3rd = ['b1_ch1', 'b2_ch2', 'b3_ch3', 'b4_ch4']\n",
    "\n",
    "col_names_set = [col_names_1st, col_names_2nd_3rd, col_names_2nd_3rd]\n",
    "select_columns = [\n",
    "        [\n",
    "            ['b1_ch1', 'b1_ch2'],\n",
    "            ['b2_ch3', 'b2_ch4'],\n",
    "            ['b3_ch5', 'b3_ch6'],\n",
    "            ['b4_ch7', 'b4_ch8']\n",
    "        ],\n",
    "        [\n",
    "            ['b1_ch1'],\n",
    "            ['b2_ch2'],\n",
    "            ['b3_ch3'],\n",
    "            ['b4_ch4'],\n",
    "        ],\n",
    "        [\n",
    "            ['b1_ch1'],\n",
    "            ['b2_ch2'],\n",
    "            ['b3_ch3'],\n",
    "            ['b4_ch4'],    \n",
    "        ],\n",
    "    ]\n",
    "\n",
    "data_set_paths=[]\n",
    "if flag_is_ec2 == True:\n",
    "    data_set_paths= [s3_objects_1st_dataset, s3_objects_2nd_dataset, s3_objects_3rd_dataset]\n",
    "\n",
    "#Verify variables\n",
    "print('Number of files in 1st Dataset:', len(data_set_paths[0]), 'first file=', data_set_paths[0][0].key)\n",
    "print('Number of files in 2nd Dataset:', len(data_set_paths[1]), 'first file=', data_set_paths[1][0].key)\n",
    "print('Number of files in 3rd Dataset:', len(data_set_paths[2]), 'first file=', data_set_paths[2][0].key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:10\u001b[0;36m\u001b[0m\n\u001b[0;31m    df[0]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "select_data_set = 0\n",
    "select_input_stepsize= 3000\n",
    "\n",
    "df = []\n",
    "if flag_is_ec2 == True:\n",
    "    s3_object = data_set_paths[0][0]\n",
    "    data = s3_object.get()['Body'].read()\n",
    "    df.append(pd.read_csv(BytesIO(data), header=None, delimiter='\\t', names=col_names_set[0], low_memory='False'))\n",
    "\n",
    "df[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
